name: Deploy to GitHub Pages

on:
  push:
    branches: [main]
  schedule:
    # 매주 월요일 KST 06:00 (UTC 21:00 일요일)에 EV 데이터 갱신 + 재빌드
    - cron: '0 21 * * 0'
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: 'pages'
  cancel-in-progress: false

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Fetch EV charger data
        env:
          EV_CHARGER_API_KEY: ${{ secrets.EV_CHARGER_API_KEY }}
        run: |
          if [ -z "$EV_CHARGER_API_KEY" ]; then
            echo "EV_CHARGER_API_KEY not configured, skipping data fetch"
            exit 0
          fi
          echo "Fetching latest EV charger data..."
          npx tsx scripts/fetch-ev-chargers.ts

      - name: Build
        run: |
          BUILD_START=$(date +%s)
          npm run build
          BUILD_END=$(date +%s)
          BUILD_DURATION=$((BUILD_END - BUILD_START))
          echo "Build completed in ${BUILD_DURATION}s"
          echo "build_duration=${BUILD_DURATION}" >> $GITHUB_OUTPUT
          # 빌드 결과 요약
          PAGE_COUNT=$(find out -name "*.html" 2>/dev/null | wc -l | tr -d ' ')
          OUT_SIZE=$(du -sh out/ 2>/dev/null | cut -f1)
          echo "Pages: ${PAGE_COUNT}, Output size: ${OUT_SIZE}"
        id: build

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: ./out

  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4

      - name: Fetch URLs from sitemap
        id: sitemap
        if: success()
        run: |
          echo "Waiting for deployment to propagate..."
          sleep 15

          # sitemap.xml에서 모든 URL 추출
          SITEMAP=$(curl -s --retry 3 --retry-delay 5 "https://car.mustarddata.com/sitemap.xml")
          if [ -z "$SITEMAP" ]; then
            echo "Failed to fetch sitemap.xml"
            echo "urls=" >> $GITHUB_OUTPUT
            exit 0
          fi

          URLS=$(echo "$SITEMAP" | grep -oP '<loc>\K[^<]+')
          URL_COUNT=$(echo "$URLS" | wc -l)
          echo "Found $URL_COUNT URLs in sitemap.xml"
          echo "$URLS" > /tmp/sitemap-urls.txt

          # IndexNow용 JSON 배열 생성
          JSON_URLS=$(echo "$URLS" | python3 -c "import sys,json; print(json.dumps([l.strip() for l in sys.stdin if l.strip()]))")
          echo "json_urls=$JSON_URLS" >> $GITHUB_OUTPUT
          echo "url_count=$URL_COUNT" >> $GITHUB_OUTPUT

      - name: Submit to IndexNow
        if: success() && steps.sitemap.outputs.url_count != ''
        run: |
          URL_COUNT=${{ steps.sitemap.outputs.url_count }}
          echo "Submitting $URL_COUNT URLs to IndexNow..."

          curl -s -X POST "https://api.indexnow.org/indexnow" \
            -H "Content-Type: application/json" \
            -d '{
              "host": "car.mustarddata.com",
              "key": "95d7b954cbdd0dbfb78b328346fcc866",
              "keyLocation": "https://car.mustarddata.com/95d7b954cbdd0dbfb78b328346fcc866.txt",
              "urlList": ${{ steps.sitemap.outputs.json_urls }}
            }' && echo "IndexNow: submitted $URL_COUNT URLs" || echo "IndexNow submission failed (non-critical)"

      - name: Submit to Google Indexing API
        if: success() && steps.sitemap.outputs.url_count != ''
        env:
          GOOGLE_SERVICE_ACCOUNT_EMAIL: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_EMAIL }}
          GOOGLE_PRIVATE_KEY: ${{ secrets.GOOGLE_PRIVATE_KEY }}
        run: |
          if [ -z "$GOOGLE_SERVICE_ACCOUNT_EMAIL" ] || [ -z "$GOOGLE_PRIVATE_KEY" ]; then
            echo "Google Indexing API credentials not configured, skipping..."
            exit 0
          fi

          # JWT 생성
          HEADER=$(echo -n '{"alg":"RS256","typ":"JWT"}' | openssl base64 -e | tr '+/' '-_' | tr -d '=\n')
          NOW=$(date +%s)
          EXP=$((NOW + 3600))
          PAYLOAD=$(echo -n "{\"iss\":\"$GOOGLE_SERVICE_ACCOUNT_EMAIL\",\"scope\":\"https://www.googleapis.com/auth/indexing\",\"aud\":\"https://oauth2.googleapis.com/token\",\"iat\":$NOW,\"exp\":$EXP}" | openssl base64 -e | tr '+/' '-_' | tr -d '=\n')
          SIGNATURE=$(echo -n "$HEADER.$PAYLOAD" | openssl dgst -sha256 -sign <(echo "$GOOGLE_PRIVATE_KEY") | openssl base64 -e | tr '+/' '-_' | tr -d '=\n')
          JWT="$HEADER.$PAYLOAD.$SIGNATURE"

          # Access Token 획득
          ACCESS_TOKEN=$(curl -s -X POST "https://oauth2.googleapis.com/token" \
            -H "Content-Type: application/x-www-form-urlencoded" \
            -d "grant_type=urn:ietf:params:oauth:grant-type:jwt-bearer&assertion=$JWT" | python3 -c "import sys,json; print(json.load(sys.stdin).get('access_token',''))")

          if [ -z "$ACCESS_TOKEN" ]; then
            echo "Failed to get access token, skipping..."
            exit 0
          fi

          # sitemap에서 추출한 URL로 제출 (일일 200개 제한 준수)
          COUNT=0
          MAX=200
          while IFS= read -r URL; do
            if [ $COUNT -ge $MAX ]; then
              echo "Reached daily limit ($MAX), stopping."
              break
            fi
            RESPONSE=$(curl -s -o /dev/null -w "%{http_code}" -X POST "https://indexing.googleapis.com/v3/urlNotifications:publish" \
              -H "Content-Type: application/json" \
              -H "Authorization: Bearer $ACCESS_TOKEN" \
              -d "{\"url\":\"$URL\",\"type\":\"URL_UPDATED\"}")
            echo "Google Indexing [$((COUNT+1))]: $URL -> $RESPONSE"
            COUNT=$((COUNT+1))
            sleep 0.1
          done < /tmp/sitemap-urls.txt
          echo "Submitted $COUNT URLs to Google Indexing API"
